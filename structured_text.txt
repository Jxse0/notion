# Business Intelligence & Analytics ‚Äì Grundlagen und praktische Anwendungen
    ## Historisch gewachsene Formen entscheidungsorientierter Datenhaltung
        - Ursprung und Zielgruppe: IT-basierte Entscheidungsunterst√ºtzung wurde zun√§chst ausschlie√ülich f√ºr F√ºhrungskr√§fte entwickelt, um dispositive Arbeitsleistungen (Leitung und Lenkung betrieblicher Vorg√§nge) zu unterst√ºtzen.
        - Datenkategorien:
            - Operative Daten: Entstehen in administrativen, dispositiven und Abrechnungssystemen, haupts√§chlich durch OLTP-Systeme (z. B. Buchungen oder Bestellungen).
            - Entscheidungsorientierte Daten (fr√ºher "dispositive Daten"): Unterst√ºtzen die Entscheidungsfindung und unterscheiden sich grundlegend von operativen Daten.
        - Herausforderungen in gewachsenen Strukturen:
            - Direkter Zugriff von entscheidungsunterst√ºtzenden Systemen auf operative Daten ist ineffizient.
            - Bis in die 1990er-Jahre erfolgte Datenhaltung in isolierten, herstellerspezifischen Systemen mit individuellen Kopien und Extrakten aus operativen und externen Quellen.
        - Nachteile dieser Ans√§tze:
            - Performanceprobleme: Wiederholtes Kopieren und Extrahieren belastet operative Systeme.
            - Inkonsistenz: Zeitlich asynchrone Datenextraktion f√ºhrt zu widerspr√ºchlichen Datenwerten.
            - Aufw√§ndigkeit und Fehleranf√§lligkeit: Harmonisierung und Verdichtung der Daten m√ºssen f√ºr jedes Subsystem separat erfolgen.
            - Datenverlustgefahr: √Ñnderungen oder L√∂schungen in vorgelagerten Systemen beeintr√§chtigen nachgelagerte Prozesse.
        - Daten-Pool-Ansatz:
            - Daten werden durch Kopieren/Extrahieren in einen dedizierten Datenpool √ºbertragen, getrennt von den Quellsystemen.
            - Vorteile:
                - Zeitliche Konsistenz der Daten.
                - Reduzierte Performance-Beeintr√§chtigung der operativen Systeme.
            - Nachteile:
                - Erforderliche Harmonisierung und Verdichtung weiterhin separat f√ºr jedes System.
                - Semantische Inkonsistenzen und keine einheitliche Datensicht.
        - Heutige Herausforderungen:
            - Viele Unternehmen replizieren die alten Ans√§tze aufgrund mangelnder √ºbergreifender BI-Strategien.
            - Dezentrale Analytics- oder Big-Data-Initiativen f√∂rdern oft isolierte Datenhaltung, die ineffizient und widerspr√ºchlich ist.
    ## Data-Warehouse-Konzept
        ### Begriff des Data Warehouse (DWH)
            Ein Data Warehouse (DWH) ist eine logisch zentralisierte, von operativen Systemen getrennte Datenhaltung, die als konsistente Datenbasis f√ºr die Entscheidungsunterst√ºtzung dient. Es wurde wesentlich von William H. Inmon definiert, der vier zentrale Merkmale beschreibt:
            1. Themenorientierung:
                - Daten sind an Entscheidungsbedarfen (z. B. Unternehmensstruktur, Kundenstruktur) ausgerichtet und nicht an operativen Prozessen.
            1. Integration:
                - Zusammenf√ºhrung unterschiedlicher Datenquellen (interne und externe), um eine widerspruchsfreie und einheitliche Datenbasis zu schaffen.
            1. Zeitraumbezug:
                - Daten decken Zeitr√§ume (z. B. Wochen, Monate) ab, wobei in modernen Systemen die Granularit√§t bis auf Transaktionsebene erweitert werden kann.
            1. Nicht-Volatilit√§t:
                - Daten im DWH sind dauerhaft gespeichert und werden nicht kontinuierlich √ºberschrieben wie in operativen Systemen. Historisierungskonzepte helfen, Datenwachstum zu managen.
        ### Architekturvarianten von DWH-/Data-Mart-L√∂sungen
            Es gibt verschiedene Ans√§tze zur Gestaltung von DWH-Architekturen, abh√§ngig von Unternehmensbedarfen und gewachsenen IT-Strukturen:
            1. Unabh√§ngige Data Marts:
                - Kleine, spezifische Datenhaltungen ohne √ºbergreifende Konsistenz. Problematisch bei √ºbergreifenden Auswertungen.
            1. Abgestimmte Data Marts:
                - Gemeinsame Datenmodelle stellen Konsistenz und Integrit√§t sicher, jedoch weiterhin mehrfacher Datenaufbereitungsaufwand.
            1. Zentrales Core-DWH (C-DWH):
                - Einheitliche, zentrale Datenbasis ohne Data Marts. Geeignet f√ºr kleinere L√∂sungen, jedoch mit Performance-Nachteilen bei gro√üen Datenvolumina.
            1. Mehrere C-DWHs:
                - Unterst√ºtzt divergierende Gesch√§ftsprozesse, besonders bei Konzernen mit mehreren Gesch√§ftseinheiten.
            1. C-DWH mit abh√§ngigen Data Marts:
                - Data Marts sind anwendungsbezogene Extrakte des C-DWH.
            1. DWH-Architektur-Mix:
                - Kombination aus C-DWHs, abh√§ngigen und unabh√§ngigen Data Marts, oft in historisch gewachsenen Umgebungen.
        ### ODS-erweiterte DWH-Architektur
            Ein Operational Data Store (ODS) erg√§nzt das DWH, indem es aktuelle, transaktionsorientierte Daten integriert, ohne sie langfristig zu historisieren.
            - ETL-Prozess (Extraction, Transformation, Loading):
                - √úberf√ºhrt Daten aus operativen Systemen und externen Quellen in das DWH, wobei die Daten harmonisiert und integriert werden.
            - Hub-and-Spoke-Architektur:
                - Core-DWH als zentrale Datenbank (Hub), von der abh√§ngige Data Marts (Spokes) abgeleitet werden.
        ### Herausforderungen und Alternativen
            - Probleme traditioneller Ans√§tze:
                - Komplexit√§t und hohe Aufw√§nde bei der Transformation und Integration.
                - Historienbetrachtung oft nicht m√∂glich in operativen Systemen.
                - Ressourcenbelastung bei Abfragen auf operativen Systemen.
            - Virtuelle DWHs:
                - Daten werden direkt aus Quellsystemen abgerufen und ad-hoc transformiert (Enterprise Information Integration, EII).
                - Noch nicht etabliert, da heterogene Systeme und externe Daten die Integration erschweren.
            - Moderne Entwicklungen:
                - Fortschritte wie In-Memory-Technologien (z. B. SAP HANA) reduzieren den Bedarf an physischen DWHs, jedoch bleiben DWHs weiterhin relevant, insbesondere bei externen und Legacy-Datenquellen.
    ## Detaillierung ODS-erweiterter Data Warehouses
        ### Transformationsprozess ‚Äì ETL
            Der ETL-Prozess (Extraction, Transformation, Loading) ist zentral, um operative Daten in entscheidungsorientierte Daten f√ºr Data Warehouses (DWH) umzuwandeln. Er umfasst die Schritte Filterung, Harmonisierung, Aggregation und Anreicherung, die zusammen die Datenqualit√§t sicherstellen und nutzbare Informationen liefern.
            Filterung
            Die Filterung selektiert relevante Daten und entfernt syntaktische sowie semantische Fehler.
            - Extraktion: Daten werden aus heterogenen Quellen in Staging Areas oder Corporate Memory √ºbertragen.
            - Bereinigung: Datenm√§ngel werden korrigiert, kategorisiert in:
                - Automatisierbare Fehler (z. B. Format- oder Wertefehler).
                - Fehler mit manueller Nachbearbeitung (z. B. durch Experten nach Plausibilit√§tspr√ºfungen).
                - Fehler, die nur von Fachspezialisten erkannt und korrigiert werden k√∂nnen.Werkzeuge wie Cleansing- und Scrubbing-Komponenten unterst√ºtzen diesen Prozess.
            Harmonisierung
            Die Harmonisierung sorgt f√ºr konsistente und vereinheitlichte Daten.
            - Syntaktische Harmonisierung:
                - Beseitigung von Schl√ºsseldisharmonien (z. B. durch Mapping-Tabellen).
                - Einheitliche Kodierung und L√∂sung von Synonymen (gleiche Bedeutung, unterschiedliche Namen) und Homonymen (gleiche Namen, unterschiedliche Bedeutung).
            - Fachliche Harmonisierung:
                - Einheitliche Definition betriebswirtschaftlicher Kennzahlen (z. B. W√§hrungen, Zeitr√§ume).
                - Festlegung der Granularit√§t f√ºr spezifische Analysen.
            Aggregation
            Die Aggregation verdichtet harmonisierte Daten, um sie f√ºr anwendungs√ºbergreifende Analysen nutzbar zu machen.
            - Dimensionshierarchien (z. B. Kunde ‚Üí Kundengruppe ‚Üí Gesamt) strukturieren die Daten.
            - Historische Konsistenz wird durch Zeitstempel gew√§hrleistet.
            - Die Aggregation verlagert Teile der Funktionalit√§t von Anwendungen in die Datenhaltung.
            Anreicherung
            Die Anreicherung berechnet betriebswirtschaftliche Kennzahlen und integriert sie in die Datenbasis.
            - Kennzahlen werden auf harmonisierten oder aggregierten Daten basierend berechnet (z. B. w√∂chentliche Deckungsbeitr√§ge).
            - Vorteile sind konsistente Werte, schnelle Abfragen und ein einheitliches betriebswirtschaftliches Instrumentarium.
            Der ETL-Prozess stellt durch Filterung, Harmonisierung, Aggregation und Anreicherung sicher, dass die Daten im Data Warehouse konsistent, hochwertig und entscheidungsrelevant sind.
        ### Core Data Warehouse und Data Marts
            Das Core Data Warehouse (C-DWH), auch Basisdatenbank genannt, ist die zentrale Datenhaltungskomponente des DWH-Konzepts. Es dient als Sammelstelle f√ºr s√§mtliche entscheidungsunterst√ºtzenden Daten und erf√ºllt wichtige Funktionen:
            - Sammel- und Integrationsfunktion: Aufnahme aller analyserelevanten Daten als logisch zentraler Speicher.
            - Distributionsfunktion: Versorgung nachgeschalteter Data Marts mit Daten.
            - Qualit√§tssicherungsfunktion: Sicherstellung der syntaktischen und semantischen Stimmigkeit transformierter Daten.
            ### Nutzung und Verwaltung
            Eine direkte Nutzung des C-DWH f√ºr Endbenutzeranalysen ist kontrovers. Anfangs wurde Power-Usern der Zugriff erm√∂glicht, doch negative Erfahrungen (z. B. durch fehlerhafte SQL-Abfragen und hohe Datenvolumina) f√ºhrten dazu, dass Analysen zunehmend in Data Marts ausgelagert werden. Heute wird das C-DWH meist ausschlie√ülich von der IT-Abteilung verwaltet.
            Das C-DWH orientiert sich an einer technischen Optimierung der Datenstrukturen, um Beladungen, Modifikationen und Weitergaben an Data Marts performant und sicher zu gew√§hrleisten. Funktionen wie Aggregation und Anreicherung stehen weniger im Fokus, es sei denn, sie sind mehrfach in verschiedenen Data Marts erforderlich.
            ### Aktualisierungsvarianten
            Daten im C-DWH werden bedarfsabh√§ngig aktualisiert, wobei drei Varianten √ºblich sind:
            - √Ñnderungsquantit√§tsbasiert: Daten√ºbertragung bei Erreichen einer definierten √Ñnderungsanzahl.
            - Periodisch: Zeitgesteuerte Aktualisierungen (z. B. st√ºndlich, t√§glich). H√§ufig verursachen sie Belastungen der operativen Systeme.
            - Echtzeit: Transaktionssynchrones Laden der Daten, was hohe Komplexit√§t und spezielle Beladungssysteme erfordert.
            ### Data Marts im Vergleich zum C-DWH
            Data Marts sind kleiner, st√§rker anwendungsorientiert und auf spezifische Benutzerkreise oder Aufgaben zugeschnitten. Sie enthalten oft:
            - Anwendungsspezifische Datenorganisation.
            - Vordefinierte Hierarchien, Aggregate und fachliche Kennziffern.
            Kritiker sehen Data Marts h√§ufig als Teil der Applikation, da sie Datenhaltung, Funktionalit√§t und Benutzeroberfl√§chen eng verzahnen. Sie werden h√§ufig mit Reporting- und OLAP-Technologien assoziiert.
            ### Technologien und Datenmodellierung
            C-DWHs basieren √ºberwiegend auf relationalen Datenbanken, die seit den 1980er-Jahren etabliert sind. Diese gelten als sicher, stabil und skalierbar.
            - Relationale Datenmodelle: Erm√∂glichen die Darstellung von Objekten und Beziehungen in zweidimensionalen Tabellen.
            - Normalisierung: Redundanz und Anomalien werden durch die Einhaltung von Codd‚Äôschen Normalformen vermieden.
            Zus√§tzlich finden in kommerziellen Ans√§tzen spaltenorientierte, In-Memory-Datenbanken sowie Big-Data- und NoSQL-Technologien zunehmend Anwendung.
        ### Operational Data Store
            Der Operational Data Store (ODS) ist ein harmonisierter Datenpool, der die operative Transaktionsverarbeitung mit der entscheidungsunterst√ºtzenden Systemlandschaft verbindet. Er erm√∂glicht taktische Entscheidungen im Tagesgesch√§ft und bietet die Grundlage f√ºr prozessorientierte Business-Intelligence-Ans√§tze.
            Charakteristika des ODS:
            - Themenorientierung: Daten werden entlang entscheidungsrelevanter Dimensionen wie Produkte, Regionen oder Kunden strukturiert.
            - Integration: Daten aus operativen Quellsystemen werden transformiert, um eine einheitliche und widerspruchsfreie Datenbasis zu schaffen. Fokus liegt auf Filterung und Harmonisierung.
            - Zeitpunktbezug: Keine langfristige Historisierung; Daten werden nur f√ºr Tage oder Wochen gespeichert, meist aus Recovery-Gr√ºnden.
            - Volatilit√§t: Daten im ODS werden regelm√§√üig aktualisiert und √ºberschrieben, wobei die Aktualisierung transaktionssynchron, st√ºndlich oder t√§glich erfolgen kann.
            - Hoher Detaillierungsgrad: Daten werden auf Transaktionsebene gespeichert, um detaillierte Analysen zu erm√∂glichen.
            Unterschiede zum Data Warehouse:
            - Im Gegensatz zum Data Warehouse (DWH) speichert das ODS keine langfristigen historischen Daten, sondern bietet zeitnahe Informationen f√ºr operative Analysen.
            - Der Fokus liegt auf aktuellen, detaillierten und volatilen Daten.
            Einsatz des ODS in der Gesch√§ftsprozessabwicklung:
            - ODS wird in Echtzeit-Anwendungen genutzt, z. B. in Call-Centern, ATMs, oder zur Lieferungsverfolgung im Supply Chain Management.
            - Es erm√∂glicht einen ganzheitlichen Blick auf Gesch√§ftsprozesse und dient der Enterprise Application Integration (EAI).
            Operational BIA:
            - Operational BIA kombiniert Business-Intelligence-Methoden mit prozessualen Ablaufdaten, um Echtzeit- oder Near-Real-Time-Unterst√ºtzung f√ºr zeitkritische Entscheidungen bereitzustellen.
            - Einsatzbeispiele:
                - Online Fraud Detection: Identifikation von Betrugsf√§llen in Versicherungen und Banken durch Abgleich aktueller Daten mit Data-Mining- und Machine-Learning-Modellen.
                - Logistik: Analysen √ºber Flugbewegungen, Gep√§cklogistik und Wetterdaten zur strategischen Planung und operativen Steuerung an Flugh√§fen.
                - Real-time CRM: In Casinos werden Kundenverhalten, Pr√§ferenzen und Loyalit√§t in Echtzeit analysiert, um personalisierte Angebote direkt w√§hrend des Spielverlaufs zu unterbreiten.
            Der ODS erweitert traditionelle DWH-Architekturen, indem er operative Daten bereitstellt, die sowohl f√ºr Echtzeitprozesse als auch zur strategischen Planung genutzt werden k√∂nnen.
        ### Metadaten, Stammdaten und Referenzdaten
            Metadaten
            Metadaten beschreiben die Bedeutung und Eigenschaften von Objekten, um deren Interpretation, Verwaltung und Nutzung zu erleichtern. Im Kontext der Datenverarbeitung umfassen sie alle Informationen, die f√ºr Analyse, Entwicklung und Betrieb eines Informationssystems notwendig sind. Im BIA-Bereich (Business Intelligence & Analytics) spielen Metadaten eine zentrale Rolle und begleiten den gesamten Lebenszyklus von BIA-Systemen.
            - Nutzungskategorien von Metadaten:
                - Passive Metadaten: Dokumentieren Struktur, Entwicklungsprozesse und Datenverwendung.
                - (Semi-)aktive Metadaten: Werden zur Laufzeit interpretiert und steuern Transformations- oder Analyseprozesse.
            - Technische und fachliche Metadaten:
                - Technische Metadaten unterst√ºtzen IT-orientierte Prozesse (z. B. Filterung).
                - Fachliche Metadaten fokussieren auf betriebswirtschaftliche Interpretationen (z. B. Harmonisierung, Anreicherung).
            Vorteile des Metadatenmanagements:
            - Effizienzsteigerung: Verbesserte Anpassung, Wartung und Wiederverwendbarkeit von Daten und Prozessen.
            - Effektivit√§t: Sicherstellung der Datenqualit√§t (Konsistenz, Aktualit√§t, Genauigkeit).
            - Berechtigungsverwaltung: Zentrale Benutzerrollen erm√∂glichen konsistente Zugriffsrechte.
            - Begriffsverst√§ndnis: Metadaten f√∂rdern eine einheitliche Terminologie (Single Point of Truth).
            Architekturvarianten des Metadatenmanagements:
            1. Zentralisiert: Alle Metadaten in einem Repository; bietet globale Konsistenz, jedoch mit Abh√§ngigkeit und Wartungsaufwand.
            1. Dezentralisiert: Lokale Repositories je Komponente; flexibel, jedoch mit Synchronisationsproblemen.
            1. F√∂deriert: Kombination aus zentralem Repository f√ºr gemeinsame Metadaten und lokalen Repositories.
            Stammdaten und Referenzdaten
            Stammdaten sind grundlegende und anwendungs√ºbergreifende Daten wie Kunden-, Produkt- oder Lieferantendaten. Referenzdaten sind Wertemengen oder Klassifikationen wie ISO-L√§ndercodes oder W√§hrungscodes, auf die Systeme verweisen.
            - Stammdatenmanagement:
                - Stammdaten sind kritisch f√ºr BIA, da sie grundlegende Analysedimensionen (z. B. Kunden, Regionen) bilden.
                - Architekturans√§tze:
                    - Peer-to-Peer-Integration: Bilaterale Schnittstellen zwischen Systemen.
                    - Zentrales Stammdatenmanagement-System: Ein zentraler Speicher f√ºr alle Systeme.
                    - F√∂derierter Ansatz: Verzeichnisdienst verweist auf g√ºltige Stammdaten in verschiedenen Systemen.
                    - F√ºhrendes System: Ein System (z. B. ERP) definiert die g√ºltigen Stammdaten.
            Herausforderungen:
            - Harmonisierung von Stammdaten ist oft aufwendig.
            - Der Betrieb eines umfassenden Stammdatenmanagements erfordert erhebliche Ressourcen, weshalb er h√§ufig nur partiell umgesetzt wird.
            Metadaten, Stammdaten und Referenzdaten sind wesentliche Bausteine f√ºr die Effizienz und Qualit√§t moderner BIA-Systeme und deren Analysen.
    ## Big Data und der Data Lake
        ### Big Data ‚Äì Begriffsabgrenzung
            Big Data beschreibt Technologien und Konzepte zur Handhabung gro√üer, heterogener und schnell entstehender Datenmengen. Die zentralen Charakteristika, bekannt als die 3Vs, sind:
            - Volume: Gro√üe Datenmengen, oft im Petabyte-Bereich.
            - Variety: Unterschiedliche Datenformate (strukturierte, semi-strukturierte und unstrukturierte Daten wie Text, Audio oder Video).
            - Velocity: Hohe Geschwindigkeit bei der Erfassung und Verarbeitung von Daten (Echtzeit- oder Near-Real-Time-Verarbeitung).
            Weitere genannte ‚ÄûVs‚Äú wie Value (Nutzen) oder Veracity (Genauigkeit) unterstreichen eher die Sinnhaftigkeit von Big-Data-L√∂sungen.
        ### Technologien und Werkzeuge
            Big-Data-L√∂sungen setzen auf horizontale Skalierung (scale-out), bei der Lasten auf viele Server verteilt werden. Statt relationaler ACID-Datenbanken (vollst√§ndige Konsistenz) kommen NoSQL-Datenbanken zum Einsatz, die BASE-Kriterien (Basic Availability, Soft State, Eventual Consistency) erf√ºllen.
            NoSQL-Datenbanktypen:
            - Key Value Stores: Einfache Schl√ºssel-Wert-Paare.
            - Document Stores: Ablage polystrukturierter Dokumente (XML, JSON).
            - Wide Column Stores: Flexible Tabellen mit variabler Spaltenanzahl.
            - Graph-Datenbanken: Speziell f√ºr vernetzte Informationen (z. B. soziale Netzwerke).
            Big-Data-Systeme setzen auf verteilte Dateisysteme, Komponenten f√ºr Datenintegration, Metadatenverwaltung (Data Catalogs) und Analysewerkzeuge.
        ### Das Konzept des Data Lake
            Ein Data Lake ist eine Big-Data-Datenhaltung, die polystrukturierte Rohdaten in ihrem Ursprungsformat speichert. Im Gegensatz zu einem Data Warehouse (DWH) erfolgt hier die Transformation der Daten erst in nachgelagerten Systemen (Schema on Read).
            Zonenstruktur eines Data Lake:
            - Transient Zone: Eingangsbereich f√ºr extrahierte Daten; erste Transformationen wie Anonymisierung.
            - Raw Data Zone: Speicherung unbearbeiteter Rohdaten.
            - Curated Zone: Bereinigte und harmonisierte Daten mit Verweisen auf Stamm- und Referenzdaten.
            - Discovery Sandbox: Direkter Zugriff f√ºr Analysten (z. B. Data Scientists).
            - Consumption Zone: Vollst√§ndig transformierte Daten f√ºr Endnutzer und Analyseanwendungen.
            Governance und Risiken:
            Ohne Konzeptualisierung und Metadatenmanagement k√∂nnen Data Lakes zu Data Swamps (unbrauchbaren Datensammlungen) verkommen.
        ### Big Data im Kontext der BIA
            Ein Data Lake erg√§nzt das klassische relationale Data Warehouse, ersetzt es jedoch nicht:
            - Data Warehouse: Eignet sich f√ºr strukturierte Daten, die hohe Konsistenz und Genauigkeit erfordern.
            - Data Lake: Speichert Rohdaten, die f√ºr komplexe Analysen (z. B. mit Deep Learning) genutzt werden.Beispiel: Bilder werden im Data Lake gespeichert; die daraus extrahierten Kategorien (z. B. Inhaltsattribute) flie√üen in das DWH f√ºr strukturierte Analysen.
    ## Anbindung der Datenbereitstellungsschicht
        
Die Datenbereitstellungsschicht erm√∂glicht den Zugriff auf die bereinigten und konsistenten Daten aus dem Data Warehouse (DWH), Data Marts oder Operational Data Stores (ODS). Je nach Anwendungsfall variieren die Anforderungen an Aktualit√§t und Reaktionszeit, was zur Entwicklung verschiedener Data-Warehousing-Ans√§tze gef√ºhrt hat.

        ### Latenzzeiten in BIA-Systemen
            Die Aktionszeit ist die Gesamtdauer von der Erfassung eines Gesch√§ftsvorfalls bis zur Umsetzung einer Ma√ünahme. Sie besteht aus:
            - Datenlatenz: Zeit f√ºr die Datenbereitstellung im DWH (Filterung, Harmonisierung, Aggregation, Anreicherung).
            - Analyselatenz: Zeit f√ºr Analyse, Aufbereitung und Bereitstellung von Informationen.
            - Entscheidungslatenz: Zeit f√ºr die Informationsaufnahme und Entscheidungsfindung.
            - Umsetzungslatenz: Zeit f√ºr die Umsetzung der Ma√ünahme.
        ### Implementierungsvarianten der Data-Warehousing-Ans√§tze
            - Klassisches Data Warehousing
                - Merkmale: Periodische ETL-Batchverarbeitung (t√§glich, w√∂chentlich, monatlich).
                - Anwendung: Ex-post-Analysen, Planungs- und Kontrollinstrumente.
                - Vorteil: Hohe Konsistenz der Daten.
                - Nachteil: Keine Optimierung der Latenzzeiten.
            - Closed-Loop Data Warehousing
                - Merkmale: R√ºckkopplung von Analyseergebnissen in operative Systeme zur Unterst√ºtzung weiterer Entscheidungen.
                - Anwendung: CRM-Anwendungen (z. B. Produktempfehlungen auf Basis von Kundensegmentierung).
                - Vorteil: Verk√ºrzt die Umsetzungslatenz durch direkte Integration der Analyseergebnisse.
            - Real-time Data Warehousing
                - Merkmale: Echtzeit-Integration der Transaktionsdaten mit minimaler Latenz.
                - Anwendung: Zeitkritische Anwendungen wie Wertpapierhandel.
                - Vorteil: Verringerung der Datenlatenz durch Abl√∂sung batchorientierter Prozesse.
                - Technologie: Einsatz von ODS und Enterprise-Application-Integration-Systemen.
            - Active Data Warehousing
                - Merkmale: Automatisierte Entscheidungsunterst√ºtzung durch die Nutzung von Regeln (Event-Condition-Action-Modell) und Data-Mining-Modellen.
                - Anwendung: Operative Prozesse wie Flugplananpassungen oder Zahlungsausfalls√ºberwachung.
                - Vorteil: Verk√ºrzt Analyse-, Entscheidungs- und Umsetzungslatenz.
            - Analyse von Streaming-Daten
                - Merkmale: Echtzeit-Analyse kontinuierlicher Datenstr√∂me (z. B. Sensor- oder Clickstream-Daten).
                - Technologie: Complex Event Processing (CEP) und Window-basierte Verarbeitung.
                - Anwendung: Operative Steuerungsszenarien wie Maschinen√ºberwachung oder Logistik-Tracking.
                - Vorteil: Erm√∂glicht nahezu verz√∂gerungsfreie Entscheidungen.
                - Architektur: Lambda- und Kappa-Architektur kombinieren Batch- und Echtzeitverarbeitung.
        ### Zusammenfassung der Implementierungsans√§tze
            Die Wahl des Ansatzes h√§ngt vom betrieblichen Bedarf ab:
            - Klassisches Data Warehousing: F√ºr planungsorientierte Anwendungen mit weniger Zeitdruck.
            - Closed-Loop Data Warehousing: F√ºr systematische R√ºckkopplung von Analyseergebnissen.
            - Real-time Data Warehousing: F√ºr zeitkritische Datenanalysen.
            - Active Data Warehousing: F√ºr automatisierte Entscheidungsfindung und -umsetzung.
            - Streaming-Daten-Verarbeitung: F√ºr Echtzeit-Analysen im Big-Data-Umfeld.
            Oft werden mehrere Ans√§tze kombiniert, um den unterschiedlichen Anforderungen gerecht zu werden. Streaming-Daten erg√§nzen dabei bestehende Architekturen, besonders im Kontext von Echtzeit- und Active-Data-Warehousing.
# üìù Lecture Notes
    ### 18/12/2024
        - Wir gehen nicht in die Tiefe, wichtig ist es das Thema zu verstehen aber als Data Scientist nimmt eher die Daten
        - Als Rolle AG erstellen wir ein Scenario als Aufgabe f√ºr eine andere Gruppe
        - Die Hausarbeit schreiben wir in der Rolle des DS
        - Thema ist das Thema der Hausarbeit
        - Gew√§hltes Thema 10 ‚ÄúEin Logistikdienstleister‚Äù gew√§hlt f√ºr die Hausarbeit und erstellen das Scenario f√ºr Thema 5
        - Wiederkehrender Informationsbedarfe z.B. Umsatz
    ### 19/12/2024
        - Entscheidungsorientierte Daten ist ein alter begriff da trifft es seiner Meinung nach Analytische Daten besser
        - Analysestrukturen
            - strukturiert: Beispiel: Wir wissen genau was wir bekommen und das wir auf jeden Fall ein Ergebnis bekommen z.B. Umsatzanalyse
        - Schema-on-
            - write: w√§re so der Data Warehouse Ansatz, ETL, brauch beim abfragen keine gro√ües Analyse know how; SQL abfragen
            - read: ben√∂tigt gr√∂√üeres Analyse know how, hab aber mehr Flexibilit√§t
        - Analyse in der Quelle
            - Bsp. ERP Systeme
            - strukturierte Analyse
            - Man muss aufpassen das man Operativen Vorg√§nge nicht st√∂rt, darum darf man nicht zu Gro√üe Operationen durchf√ºhren
            - Man hat nur diese eine Quelle
            - Daten sind hoch aktuell
        - Data Warehouse
            - strukturierte Analyse
            - Historische Daten
        - Data Mart
            - kann man schon fast kleines Data Warehouse nennen
            - unabh√§ngige Data Marts gr√ºnde:
                - man hat halt mit data marts angefangen 
                - vielleicht ist der transformierte Datenbestand so komplex ist das wir ein Data Mart machen und dann erst ins Data Warehouse (also optional ins Data Warehouse)
        - Operational Data Store
            - Sehr Aktuell
            - Sinnvoll wenn es mehr als eine Quelle zu betrachten ist, kannst sonst ja direkt ins Quellsystem
            - Historisierung ist nicht wichtig
            - z.B. B√§ckerei: Lagerbestand, Schichtbesetzung, Vorbestellung ‚Üí Wie viele Backwaren k√∂nnen und sollten wir morgen produzieren?
        - Erg√§nzende Architekturkomponente 
            - K√∂nnen unterschiedliche Server sein
            - Staging Area
                - Sammlung der Rohdaten
            - Cleansing Area
                - Transformation
            - Archivierung
        - Data Lake
            - All m√∂gliche Datenstrukturen
            - Analysen sind Unstrukturiert bzw semi strukturiert
        - Real-Time Processing
            - Lambda-Architektur
                - Stromdaten gehen in ein Ingestion Layer wo die Daten dann aufbereitet gespeichert werden und in einem anderem weg werden die daten direkt weiter zum server gesendet Unaufbereitet
            - Kappa-Architektur
                - Daten werden Aufbereitet
        - Metadata Repository (Data Catalog als synonym)
            - Am besten ein einziges Metadata Repository aber h√§ufig gar nicht m√∂glich
            - Created, woher kommst du, how to use
        - Sandbox
            - Reinwerfen der Rohdaten
        - Data Lakehouse
            - unterschied zum Data Lake ist  das man auch Strukturierte Analysen machen kann
            - = Data Lake + Data Warehouse
            - Folie 45: d ist eigentlich nichts anderes wie c 
            - Folie 46 Rechtes Bild: man kann sagen
                - Bronze: Staging
                - Silver: Data Warehouse
                - Gold: Data Mart
        - Data Fabric 
            - Daten werden nicht gespeichert sondern nur aufbereitet
            - Als einfaches Beispiel w√§re es sowas wie eine View oder Materialized View
        - Data Product (Data Mesh)
            - Fachbereiche bekommen die M√∂glichkeit ihre eigenen Datent√∂pfen zu erstellen
            - Data Mesh = Kombination von Data Product
            - Data Product soll maximal klein sein
            - also jeder baut gef√ºhlt sein eigenes Datenset und wenn jemand was davon braucht dann m√ºssen Sie sich vernetztend
# Scenario
    - wo landen die echtzeit daten? senoren, gps an einem ort oder unterschiedlichen orten‚áí unterschiedliche orten  
    - ERP: 
        - Lagerbestand
        - Lieferdaten
        - Buchhaltung
    - CRM:
        - Kundendaten
        - manuell feedback sammeln und dann anh√§ngen, skala
    - GPS vom shipping
    - Temperatur Sensoren f√ºr die Lagerung im LKW sowie im lager, gestreamt, alert wenn was ist
    - Dom√§nen: Vertrieb, Lager, Transport, HR, IT, Verwaltung
    - lkw standort aktuell aber nicht gestreatm
    - prduktspezifische anforderungen als unstruktiert oder semi